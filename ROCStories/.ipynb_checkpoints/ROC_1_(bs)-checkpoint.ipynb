{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Encoder\n",
    "Exactly as in pytorch port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ftfy\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"\n",
    "    Return set of symbol pairs in a word.\n",
    "    word is represented as tuple of symbols (symbols being variable-length strings)\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "def text_standardize(text):\n",
    "    \"\"\"\n",
    "    fixes some issues the spacy tokenizer had on books corpus\n",
    "    also does some whitespace standardization\n",
    "    \"\"\"\n",
    "    text = text.replace('—', '-')\n",
    "    text = text.replace('–', '-')\n",
    "    text = text.replace('―', '-')\n",
    "    text = text.replace('…', '...')\n",
    "    text = text.replace('´', \"'\")\n",
    "    text = re.sub(r'''(-+|~+|!+|\"+|;+|\\?+|\\++|,+|\\)+|\\(+|\\\\+|\\/+|\\*+|\\[+|\\]+|}+|{+|\\|+|_+)''', r' \\1 ', text)\n",
    "    text = re.sub(r'\\s*\\n\\s*', ' \\n ', text)\n",
    "    text = re.sub(r'[^\\S\\n]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "class TextEncoder(object):\n",
    "    \"\"\"\n",
    "    mostly a wrapper for a public python bpe tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_path, bpe_path):\n",
    "        self.nlp = spacy.load('en', disable=['parser', 'tagger', 'ner', 'textcat'])\n",
    "        self.encoder = json.load(open(encoder_path))\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        merges = open(bpe_path, encoding='utf-8').read().split('\\n')[1:-1]\n",
    "        merges = [tuple(merge.split()) for merge in merges]\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "    def bpe(self, token):\n",
    "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token+'</w>'\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        if word == '\\n  </w>':\n",
    "            word = '\\n</w>'\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, texts, verbose=True):\n",
    "        texts_tokens = []\n",
    "        if verbose:\n",
    "            for text in tqdm(texts, ncols=80, leave=False):\n",
    "                text = self.nlp(text_standardize(ftfy.fix_text(text)))\n",
    "                text_tokens = []\n",
    "                for token in text:\n",
    "                    text_tokens.extend([self.encoder.get(t, 0) for t in self.bpe(token.text.lower()).split(' ')])\n",
    "                texts_tokens.append(text_tokens)\n",
    "        else:\n",
    "            for text in texts:\n",
    "                text = self.nlp(text_standardize(ftfy.fix_text(text)))\n",
    "                text_tokens = []\n",
    "                for token in text:\n",
    "                    text_tokens.extend([self.encoder.get(t, 0) for t in self.bpe(token.text.lower()).split(' ')])\n",
    "                texts_tokens.append(text_tokens)\n",
    "        return texts_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder('model/encoder_bpe_40000.json', 'model/vocab_40000.bpe')\n",
    "encoder = text_encoder.encoder\n",
    "n_vocab = len(text_encoder.encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Same as pytorch port for ROCStories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(*splits, encoder):\n",
    "    encoded_splits = []\n",
    "    for split in splits:\n",
    "        fields = []\n",
    "        for field in split:\n",
    "            if isinstance(field[0], str):\n",
    "                field = encoder.encode(field)\n",
    "            fields.append(field)\n",
    "        encoded_splits.append(fields)\n",
    "    return encoded_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 3535999445\n",
    "\n",
    "def _rocstories(path):\n",
    "    with open(path, encoding='utf_8') as f:\n",
    "        f = csv.reader(f)\n",
    "        st = []\n",
    "        ct1 = []\n",
    "        ct2 = []\n",
    "        y = []\n",
    "        for i, line in enumerate(tqdm(list(f), ncols=80, leave=False)):\n",
    "            if i > 0:\n",
    "                s = ' '.join(line[1:5])\n",
    "                c1 = line[5]\n",
    "                c2 = line[6]\n",
    "                st.append(s)\n",
    "                ct1.append(c1)\n",
    "                ct2.append(c2)\n",
    "                y.append(int(line[-1])-1)\n",
    "        return st, ct1, ct2, y\n",
    "\n",
    "def rocstories(data_dir, n_train=1497, n_valid=374):\n",
    "    storys, comps1, comps2, ys = _rocstories(os.path.join(data_dir, 'cloze_test_val__spring2016 - cloze_test_ALL_val.csv'))\n",
    "    teX1, teX2, teX3, teY = _rocstories(os.path.join(data_dir, 'cloze_test_test__spring2016 - cloze_test_ALL_test.csv'))\n",
    "    tr_storys, va_storys, tr_comps1, va_comps1, tr_comps2, va_comps2, tr_ys, va_ys = train_test_split(storys, comps1, comps2, ys, test_size=n_valid, random_state=seed)\n",
    "    trX1, trX2, trX3 = [], [], []\n",
    "    trY = []\n",
    "    for s, c1, c2, y in zip(tr_storys, tr_comps1, tr_comps2, tr_ys):\n",
    "        trX1.append(s)\n",
    "        trX2.append(c1)\n",
    "        trX3.append(c2)\n",
    "        trY.append(y)\n",
    "\n",
    "    vaX1, vaX2, vaX3 = [], [], []\n",
    "    vaY = []\n",
    "    for s, c1, c2, y in zip(va_storys, va_comps1, va_comps2, va_ys):\n",
    "        vaX1.append(s)\n",
    "        vaX2.append(c1)\n",
    "        vaX3.append(c2)\n",
    "        vaY.append(y)\n",
    "    trY = np.asarray(trY, dtype=np.int32)\n",
    "    vaY = np.asarray(vaY, dtype=np.int32)\n",
    "    teY = np.asarray(teY, dtype=np.int32)\n",
    "    return (trX1, trX2, trX3, trY), (vaX1, vaX2, vaX3, vaY), (teX1, teX2, teX3, teY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "((trX1, trX2, trX3, trY),\n",
    " (vaX1, vaX2, vaX3, vaY),\n",
    " (teX1, teX2, teX3, teY)) = encode_dataset(*rocstories('data/', n_valid=374),\n",
    "                                      encoder=text_encoder)\n",
    "np.save('data/trY.npy', trY)\n",
    "np.save('data/vaY.npy', vaY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder['_start_'] = len(encoder)\n",
    "encoder['_delimiter_'] = len(encoder)\n",
    "encoder['_classify_'] = len(encoder)\n",
    "clf_token = encoder['_classify_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ctx = 512\n",
    "n_special = 3\n",
    "max_len = n_ctx // 2 - 2\n",
    "n_ctx = min(max(\n",
    "    [len(x1[:max_len]) + max(len(x2[:max_len]),\n",
    "                             len(x3[:max_len])) for x1, x2, x3 in zip(trX1, trX2, trX3)]\n",
    "    + [len(x1[:max_len]) + max(len(x2[:max_len]),\n",
    "                               len(x3[:max_len])) for x1, x2, x3 in zip(vaX1, vaX2, vaX3)]\n",
    "    + [len(x1[:max_len]) + max(len(x2[:max_len]),\n",
    "                               len(x3[:max_len])) for x1, x2, x3 in zip(teX1, teX2, teX3)]\n",
    "    ) + 3, n_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = n_vocab + n_special + n_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_roc(X1, X2, X3):\n",
    "    n_batch = len(X1)\n",
    "    xmb = np.zeros((n_batch, 2, n_ctx, 2), dtype=np.int32)\n",
    "    mmb = np.zeros((n_batch, 2, n_ctx), dtype=np.float32)\n",
    "    start = encoder['_start_']\n",
    "    delimiter = encoder['_delimiter_']\n",
    "    for i, (x1, x2, x3), in enumerate(zip(X1, X2, X3)):\n",
    "        x12 = [start] + x1[:max_len] + [delimiter] + x2[:max_len] + [clf_token]\n",
    "        x13 = [start] + x1[:max_len] + [delimiter] + x3[:max_len] + [clf_token]\n",
    "        l12 = len(x12)\n",
    "        l13 = len(x13)\n",
    "        xmb[i, 0, :l12, 0] = x12\n",
    "        xmb[i, 1, :l13, 0] = x13\n",
    "        mmb[i, 0, :l12] = 1\n",
    "        mmb[i, 1, :l13] = 1\n",
    "    # Position information that is added to the input embeddings in the TransformerModel\n",
    "    xmb[:, :, :, 1] = np.arange(n_vocab + n_special, n_vocab + n_special + n_ctx)\n",
    "    return xmb, mmb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this once to create and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trX, trM = transform_roc(trX1, trX2, trX3)\n",
    "vaX, vaM = transform_roc(vaX1, vaX2, vaX3)\n",
    "teX, teM = transform_roc(teX1, teX2, teX3)\n",
    "np.save('data/trX.npy', trX)\n",
    "np.save('data/trM.npy', trM)\n",
    "np.save('data/vaX.npy', vaX)\n",
    "np.save('data/vaM.npy', vaM)\n",
    "np.save('data/teX.npy', teX)\n",
    "np.save('data/teM.npy', teM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1871, 2, 77, 2)\n"
     ]
    }
   ],
   "source": [
    "print(teX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this subsequently to load everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trX = np.load('data/trX.npy')\n",
    "trM = np.load('data/trM.npy')\n",
    "vaX = np.load('data/vaX.npy')\n",
    "vaM = np.load('data/vaM.npy')\n",
    "teX = np.load('data/teX.npy')\n",
    "teM = np.load('data/teM.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1497, 2, 77, 2)\n"
     ]
    }
   ],
   "source": [
    "print(trX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(trY)\n",
    "n_valid = len(vaY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch_train = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast ai style dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/fas/radev/jbk54/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from fastai.text import *\n",
    "from fastai.dataloader import DataLoader\n",
    "from fastai.dataset import *\n",
    "\n",
    "class RocDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x,self.y=x,y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        return np.array(x),self.y[idx]\n",
    "\n",
    "    def __len__(self): return len(self.x)\n",
    "\n",
    "trn_ds = RocDataset(trX, trY)\n",
    "val_ds = RocDataset(vaX, vaY)\n",
    "tst_ds = RocDataset(teX, teY)\n",
    "trn_samp = SortishSampler(trX, key=lambda x: len(trX[x]), bs=8)\n",
    "val_samp = SortSampler(vaX, key=lambda x: len(vaX[x]))\n",
    "tst_samp = SortSampler(teX, key=lambda x: len(teX[x]))\n",
    "trn_dl = DataLoader(trn_ds, 8, num_workers=1, pad_idx=0, sampler=trn_samp)\n",
    "val_dl = DataLoader(val_ds, 8, num_workers=1, pad_idx=0, sampler=val_samp)\n",
    "tst_dl = DataLoader(tst_ds, 1, num_workers=1, pad_idx=0, sampler=tst_samp)\n",
    "PATH = Path('data')\n",
    "md = ModelData(PATH, trn_dl, val_dl, tst_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "direct from model_pytorch.py from pytorch port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "ACT_FNS = {\n",
    "    'relu': nn.ReLU,\n",
    "    'swish': swish,\n",
    "    'gelu': gelu\n",
    "}\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module in the OpenAI style (epsilon inside the square root).\"\n",
    "\n",
    "    def __init__(self, n_state, e=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.g = nn.Parameter(torch.ones(n_state))\n",
    "        self.b = nn.Parameter(torch.zeros(n_state))\n",
    "        self.e = e\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.e)\n",
    "        return self.g * x + self.b\n",
    "\n",
    "\n",
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, nf, rf, nx):\n",
    "        super(Conv1D, self).__init__()\n",
    "        self.rf = rf\n",
    "        self.nf = nf\n",
    "        if rf == 1:  # faster 1x1 conv\n",
    "            w = torch.empty(nx, nf)\n",
    "            nn.init.normal_(w, std=0.02)\n",
    "            self.w = Parameter(w)\n",
    "            self.b = Parameter(torch.zeros(nf))\n",
    "        else:  # was used to train LM\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.rf == 1:\n",
    "            size_out = x.size()[:-1] + (self.nf,)\n",
    "            x = torch.addmm(self.b, x.view(-1, x.size(-1)), self.w)\n",
    "            x = x.view(*size_out)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, nx, n_ctx, cfg, scale=False):\n",
    "        super(Attention, self).__init__()\n",
    "        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n",
    "        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n",
    "        assert n_state % cfg.n_head == 0\n",
    "        self.register_buffer('b', torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "        self.n_head = cfg.n_head\n",
    "        self.split_size = n_state\n",
    "        self.scale = scale\n",
    "        self.c_attn = Conv1D(n_state * 3, 1, nx)\n",
    "        self.c_proj = Conv1D(n_state, 1, nx)\n",
    "        self.attn_dropout = nn.Dropout(cfg.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(cfg.resid_pdrop)\n",
    "\n",
    "    def _attn(self, q, k, v):\n",
    "        w = torch.matmul(q, k)\n",
    "        if self.scale:\n",
    "            w = w / math.sqrt(v.size(-1))\n",
    "        w = w * self.b + -1e9 * (1 - self.b)  # TF implem method: mask_attn_weights\n",
    "        w = nn.Softmax(dim=-1)(w)\n",
    "        w = self.attn_dropout(w)\n",
    "        return torch.matmul(w, v)\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
    "        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n",
    "\n",
    "    def split_heads(self, x, k=False):\n",
    "        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
    "        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n",
    "        if k:\n",
    "            return x.permute(0, 2, 3, 1)\n",
    "        else:\n",
    "            return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_attn(x)\n",
    "        query, key, value = x.split(self.split_size, dim=2)\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key, k=True)\n",
    "        value = self.split_heads(value)\n",
    "        a = self._attn(query, key, value)\n",
    "        a = self.merge_heads(a)\n",
    "        a = self.c_proj(a)\n",
    "        a = self.resid_dropout(a)\n",
    "        return a\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_state, cfg):  # in MLP: n_state=3072 (4 * n_embd)\n",
    "        super(MLP, self).__init__()\n",
    "        nx = cfg.n_embd\n",
    "        self.c_fc = Conv1D(n_state, 1, nx)\n",
    "        self.c_proj = Conv1D(nx, 1, n_state)\n",
    "        self.act = ACT_FNS[cfg.afn]\n",
    "        self.dropout = nn.Dropout(cfg.resid_pdrop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.act(self.c_fc(x))\n",
    "        h2 = self.c_proj(h)\n",
    "        return self.dropout(h2)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_ctx, cfg, scale=False):\n",
    "        super(Block, self).__init__()\n",
    "        nx = cfg.n_embd\n",
    "        self.attn = Attention(nx, n_ctx, cfg, scale)\n",
    "        self.ln_1 = LayerNorm(nx)\n",
    "        self.mlp = MLP(4 * nx, cfg)\n",
    "        self.ln_2 = LayerNorm(nx)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.attn(x)\n",
    "        n = self.ln_1(x + a)\n",
    "        m = self.mlp(n)\n",
    "        h = self.ln_2(n + m)\n",
    "        return h\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\" Transformer model \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, vocab=40990, n_ctx=512):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.embed = nn.Embedding(vocab, cfg.n_embd)\n",
    "        self.drop = nn.Dropout(cfg.embd_pdrop)\n",
    "        block = Block(n_ctx, cfg, scale=True)\n",
    "        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(cfg.n_layer)])\n",
    "\n",
    "        nn.init.normal_(self.embed.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.size(-2), x.size(-1))\n",
    "        e = self.embed(x)\n",
    "        # Add the position information to the input embeddings\n",
    "        h = e.sum(dim=2)\n",
    "        for block in self.h:\n",
    "            h = block(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class LMHead(nn.Module):\n",
    "    \"\"\" Language Model Head for the transformer \"\"\"\n",
    "\n",
    "    def __init__(self, model, cfg):\n",
    "        super(LMHead, self).__init__()\n",
    "        self.n_embd = cfg.n_embd\n",
    "        embed_shape = model.embed.weight.shape\n",
    "        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n",
    "        self.decoder.weight = model.embed.weight # Tied weights\n",
    "\n",
    "    def forward(self, h):\n",
    "        # Truncated Language modeling logits (we remove the last token)\n",
    "        h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd)\n",
    "        lm_logits = self.decoder(h_trunc)\n",
    "        return lm_logits\n",
    "\n",
    "\n",
    "class MultipleChoiceHead(nn.Module):\n",
    "    \"\"\" Classifier Head for the transformer \"\"\"\n",
    "\n",
    "    def __init__(self, clf_token, cfg):\n",
    "        super(MultipleChoiceHead, self).__init__()\n",
    "        self.n_embd = cfg.n_embd\n",
    "        self.clf_token = clf_token\n",
    "        self.dropout = nn.Dropout2d(cfg.clf_pdrop)  # To reproduce the noise_shape parameter of TF implementation\n",
    "        self.linear = nn.Linear(cfg.n_embd, 1)\n",
    "\n",
    "        nn.init.normal_(self.linear.weight, std = 0.02)\n",
    "        nn.init.normal_(self.linear.bias, 0)\n",
    "\n",
    "    def forward(self, h, x):\n",
    "        # Classification logits\n",
    "        clf_h = h.view(-1, self.n_embd)\n",
    "        flat = x[..., 0].contiguous().view(-1)\n",
    "        clf_h = clf_h[flat == self.clf_token, :]\n",
    "        clf_h = clf_h.view(-1, x.size(1), self.n_embd, 1)\n",
    "        # This double transposition is there to replicate the behavior\n",
    "        # of the noise_shape argument in the tensorflow\n",
    "        # implementation.  For more details, see\n",
    "        # https://github.com/huggingface/pytorch-openai-transformer-lm/issues/11\n",
    "        clf_h = self.dropout(clf_h.transpose(1, 2)).transpose(1, 2)\n",
    "        clf_h = clf_h.contiguous().view(-1, self.n_embd)\n",
    "        clf_logits = self.linear(clf_h)\n",
    "\n",
    "        return clf_logits.view(-1, x.size(1))\n",
    "\n",
    "\n",
    "class ClfHead(nn.Module):\n",
    "    \"\"\"Classification Head for the transformer\n",
    "    TODO: test this class.\"\"\"\n",
    "    def __init__(self, clf_token, cfg, n_class):\n",
    "        super(ClfHead, self).__init__()\n",
    "        self.n_embd = cfg.n_embd\n",
    "        self.clf_token = clf_token\n",
    "        self.dropout = nn.Dropout(cfg.clf_pdrop)\n",
    "        self.linear = nn.Linear(cfg.n_embd, n_class)\n",
    "\n",
    "        nn.init.normal_(self.linear.weight, std = 0.02)\n",
    "        nn.init.normal_(self.linear.bias, 0)\n",
    "\n",
    "    def forward(self, h, x):\n",
    "        clf_h = h.view(-1, self.n_embd)\n",
    "        flat = x[..., 0].contiguous().view(-1)\n",
    "        clf_h = clf_h[flat == self.clf_token, :]\n",
    "        clf_h = self.dropout(clf_h)\n",
    "        clf_logits = self.linear(clf_h)\n",
    "\n",
    "        return clf_logits\n",
    "\n",
    "class SimilarityHead(nn.Module):\n",
    "    \"\"\" Similarity Head for the transformer\n",
    "        TODO: test this class.\"\"\"\n",
    "    def __init__(self, clf_token, cfg):\n",
    "        super(SimilarityHead, self).__init__()\n",
    "        self.n_embd = cfg.n_embd\n",
    "        self.clf_token = clf_token\n",
    "        self.dropout = nn.Dropout(cfg.clf_pdrop)\n",
    "        self.linear = nn.Linear(cfg.n_embd, 1)\n",
    "\n",
    "        nn.init.normal_(self.linear.weight, std = 0.02)\n",
    "        nn.init.normal_(self.linear.bias, 0)\n",
    "\n",
    "    def forward(self, h, x):\n",
    "        sim_h = h.view(-1, self.n_embd)\n",
    "        flat = x[..., 0].contiguous().view(-1)\n",
    "        sim_h = sim_h[flat == self.clf_token, :]\n",
    "        sim_h = self.dropout(sim_h)\n",
    "        sim_h = sim_h.sum(dim = 1)\n",
    "        sim_logits = self.linear(sim_h)\n",
    "\n",
    "        return sim_logits\n",
    "\n",
    "class DoubleHeadModel(nn.Module):\n",
    "    \"\"\" Transformer with language model and task specific heads \"\"\"\n",
    "    def __init__(self, cfg, clf_token, task_head_type, vocab=40990, n_ctx=512):\n",
    "        super(DoubleHeadModel, self).__init__()\n",
    "        self.transformer = TransformerModel(cfg, vocab=vocab, n_ctx=n_ctx)\n",
    "        self.lm_head = LMHead(self.transformer, cfg)\n",
    "        if isinstance(task_head_type, str):\n",
    "            if task_head_type == 'multiple_choice':\n",
    "                self.task_head = MultipleChoiceHead(clf_token, cfg)\n",
    "            elif task_head_type == 'similarity':\n",
    "                self.task_head = SimilarityHead(clf_token, cfg)\n",
    "            elif task_head_type == 'inference':\n",
    "                # the three classes correspond to entailment, contradiction and neutral.\n",
    "                self.task_head = ClfHead(clf_token, cfg, 3)\n",
    "            else:\n",
    "                raise ValueError(\"task_head_type is expected to be 'multiple_choice' \"\n",
    "                                 \"'similarity', 'inference' or ('classification', n_class) \"\n",
    "                                 f\"got {task_head_type}.\")\n",
    "        elif isinstance(task_head_type, collections.abc.Sequence) and len(task_head_type) == 2 and \\\n",
    "             task_head_type[0] == 'classification':\n",
    "            n_class = task_head_type[1]\n",
    "            self.task_head = ClfHead(clf_token, cfg, n_class)\n",
    "        else:\n",
    "            raise ValueError(\"task_head_type is expected to be 'multiple_choice' \"\n",
    "                             \"'similarity', 'inference' or ('classification', n_class) \"\n",
    "                             f\"got {task_head_type}.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.transformer(x)\n",
    "        lm_logits = self.lm_head(h)\n",
    "        task_logits = self.task_head(h, x)\n",
    "\n",
    "        return lm_logits, task_logits\n",
    "\n",
    "\n",
    "def load_openai_pretrained_model(model, n_ctx=-1, n_special=-1, n_transfer=12, n_embd=768, path='./model/',\n",
    "                                 path_names='./'):\n",
    "    # Load weights from TF model\n",
    "    print(\"Loading weights...\")\n",
    "    names = json.load(open(path_names + 'parameters_names.json'))\n",
    "    shapes = json.load(open(path + 'params_shapes.json'))\n",
    "    offsets = np.cumsum([np.prod(shape) for shape in shapes])\n",
    "    init_params = [np.load(path + 'params_{}.npy'.format(n)) for n in range(10)]\n",
    "    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]\n",
    "    init_params = [param.reshape(shape) for param, shape in zip(init_params, shapes)]\n",
    "    if n_ctx > 0:\n",
    "        init_params[0] = init_params[0][:n_ctx]\n",
    "    if n_special > 0:\n",
    "        init_params[0] = np.concatenate(\n",
    "            [init_params[1],\n",
    "             (np.random.randn(n_special, n_embd) * 0.02).astype(np.float32),\n",
    "             init_params[0]\n",
    "             ], 0)\n",
    "    else:\n",
    "        init_params[0] = np.concatenate(\n",
    "            [init_params[1],\n",
    "             init_params[0]\n",
    "             ], 0)\n",
    "    del init_params[1]\n",
    "    if n_transfer == -1:\n",
    "        n_transfer = 0\n",
    "    else:\n",
    "        n_transfer = 1 + n_transfer * 12\n",
    "    init_params = [arr.squeeze() for arr in init_params]\n",
    "\n",
    "    try:\n",
    "        assert model.embed.weight.shape == init_params[0].shape\n",
    "    except AssertionError as e:\n",
    "        e.args += (model.embed.weight.shape, init_params[0].shape)\n",
    "        raise\n",
    "\n",
    "    model.embed.weight.data = torch.from_numpy(init_params[0])\n",
    "\n",
    "    for name, ip in zip(names[1:n_transfer], init_params[1:n_transfer]):\n",
    "        name = name[6:]  # skip \"model/\"\n",
    "        assert name[-2:] == \":0\"\n",
    "        name = name[:-2]\n",
    "        name = name.split('/')\n",
    "        pointer = model\n",
    "        for m_name in name:\n",
    "            if re.fullmatch(r'[A-Za-z]+\\d+', m_name):\n",
    "                l = re.split(r'(\\d+)', m_name)\n",
    "            else:\n",
    "                l = [m_name]\n",
    "            pointer = getattr(pointer, l[0])\n",
    "            if len(l) >= 2:\n",
    "                num = int(l[1])\n",
    "                pointer = pointer[num]\n",
    "        try:\n",
    "            assert pointer.shape == ip.shape\n",
    "        except AssertionError as e:\n",
    "            e.args += (pointer.shape, ip.shape)\n",
    "            raise\n",
    "        pointer.data = torch.from_numpy(ip)\n",
    "\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "DEFAULT_CONFIG = dotdict({\n",
    "    'n_embd': 768,\n",
    "    'n_head': 12,\n",
    "    'n_layer': 12,\n",
    "    'embd_pdrop': 0.1,\n",
    "    'attn_pdrop': 0.1,\n",
    "    'resid_pdrop': 0.1,\n",
    "    'afn': 'gelu',\n",
    "    'clf_pdrop': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create model, load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_model = DoubleHeadModel(DEFAULT_CONFIG, clf_token, 'multiple_choice', vocab, n_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights...\n"
     ]
    }
   ],
   "source": [
    "load_openai_pretrained_model(dh_model.transformer, n_ctx=n_ctx, n_special=n_special)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I use the OpenAI optimizer directly from the pytorch port. It's AdamW, but it has to be initialized with the total number of iterations because it internally updates the learning rate and parameters. I'd like to re-do this part using fast.ai's implementation of AdamW and training phase API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def warmup_cosine(x, warmup=0.002):\n",
    "    s = 1 if x <= warmup else 0\n",
    "    return s*(x/warmup) + (1-s)*(0.5 * (1 + torch.cos(math.pi * x)))\n",
    "\n",
    "def warmup_constant(x, warmup=0.002):\n",
    "    s = 1 if x <= warmup else 0\n",
    "    return s*(x/warmup) + (1-s)*1\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    s = 1 if x <= warmup else 0\n",
    "    return (s*(x/warmup) + (1-s))*(1-x)\n",
    "\n",
    "SCHEDULES = {\n",
    "    'warmup_cosine':warmup_cosine,\n",
    "    'warmup_constant':warmup_constant,\n",
    "    'warmup_linear':warmup_linear,\n",
    "}\n",
    "\n",
    "\n",
    "class OpenAIAdam(Optimizer):\n",
    "    \"\"\"Implements Open AI version of Adam algorithm with weight decay fix.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr, schedule, warmup, t_total,\n",
    "                 b1=0.9, b2=0.999, e=1e-8, l2=0,\n",
    "                 vector_l2=False, max_grad_norm=-1, **kwargs):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if schedule not in SCHEDULES:\n",
    "            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\n",
    "        if not 0 <= warmup:\n",
    "            raise ValueError(\"Invalid warmup: {}\".format(warmup))\n",
    "        if not 0.0 <= b1 < 1.0:\n",
    "            raise ValueError(\"Invalid b1 parameter: {}\".format(b1))\n",
    "        if not 0.0 <= b2 < 1.0:\n",
    "            raise ValueError(\"Invalid b2 parameter: {}\".format(b2))\n",
    "        if not 0.0 <= e:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(e))\n",
    "        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,\n",
    "                        b1=b1, b2=b2, e=e, l2=l2, vector_l2=vector_l2,\n",
    "                        max_grad_norm=max_grad_norm)\n",
    "        super(OpenAIAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['b1'], group['b2']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # Add grad clipping\n",
    "                if group['max_grad_norm'] > 0:\n",
    "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                denom = exp_avg_sq.sqrt().add_(group['e'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "                schedule_fct = SCHEDULES[group['schedule']]\n",
    "                lr_scheduled = group['lr'] * schedule_fct(state['step']/group['t_total'], group['warmup'])\n",
    "                step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                # Add weight decay at the end (fixed version)\n",
    "                if (len(p.size()) > 1 or group['vector_l2']) and group['l2'] > 0:\n",
    "                    p.data.add_(-lr_scheduled * group['l2'], p.data)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(OpenAIAdam, lr=6.25e-5, schedule='warmup_linear', warmup=0.002, t_total=561)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create learner\n",
    "\n",
    "try adding model besides singlemodel for discriminative learning rates (get_layer_groups)? openai has some kind of structure for \"loading until\" - \"n_transfer\" parameter for load_openai_pretrained model - so they differentiate the layers somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(md, SingleModel(to_gpu(dh_model)), opt_fn=opt_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stepper and Loss\n",
    "\n",
    "custom stepper to feed into our loss\n",
    "\n",
    "loss follows closely the loss from openAI port but need to examine ignore_idx vs. masking, averaging, etc. still"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocStepper():\n",
    "    def __init__(self, m, opt, crit, clip=0, reg_fn=None, fp16=False, loss_scale=1):\n",
    "        self.m,self.opt,self.crit,self.clip,self.reg_fn = m,opt,crit,clip,reg_fn\n",
    "        self.fp16 = fp16\n",
    "        self.reset(True)\n",
    "        if self.fp16: self.fp32_params = copy_model_to_fp32(m, opt)\n",
    "        self.loss_scale = loss_scale\n",
    "\n",
    "    def reset(self, train=True):\n",
    "        if train: apply_leaf(self.m, set_train_mode)\n",
    "        else: self.m.eval()\n",
    "        if hasattr(self.m, 'reset'):\n",
    "            self.m.reset()\n",
    "            if self.fp16: self.fp32_params = copy_model_to_fp32(self.m, self.opt)\n",
    "\n",
    "    def step(self, xs, y, epoch):\n",
    "        xtra = []\n",
    "        lm_logits, task_logits = self.m(*xs)\n",
    "        #loss = raw_loss = self.crit(lm_logits)\n",
    "        #if isinstance(output,tuple): lm_logits, task_logits = output\n",
    "        if self.fp16: self.m.zero_grad()\n",
    "        else: self.opt.zero_grad() \n",
    "        loss = raw_loss = self.crit(lm_logits, task_logits, *xs, y)\n",
    "        if self.loss_scale != 1: assert(self.fp16); loss = loss*self.loss_scale\n",
    "        if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n",
    "        loss.backward()\n",
    "        if self.fp16: update_fp32_grads(self.fp32_params, self.m)\n",
    "        if self.loss_scale != 1:\n",
    "            for param in self.fp32_params: param.grad.data.div_(self.loss_scale)\n",
    "        if self.clip:   # Gradient clipping\n",
    "            if IS_TORCH_04: nn.utils.clip_grad_norm_(trainable_params_(self.m), self.clip)\n",
    "            else: nn.utils.clip_grad_norm(trainable_params_(self.m), self.clip)\n",
    "        if 'wd' in self.opt.param_groups[0] and self.opt.param_groups[0]['wd'] != 0: \n",
    "            #Weight decay out of the loss. After the gradient computation but before the step.\n",
    "            for group in self.opt.param_groups:\n",
    "                lr, wd = group['lr'], group['wd']\n",
    "                for p in group['params']:\n",
    "                    if p.grad is not None: p.data = p.data.add(-wd * lr, p.data)\n",
    "        self.opt.step()\n",
    "        if self.fp16: \n",
    "            copy_fp32_to_model(self.m, self.fp32_params)\n",
    "            torch.cuda.synchronize()\n",
    "        return torch_item(raw_loss.data)\n",
    "    \n",
    "    def evaluate(self, xs, y):\n",
    "        lm_logits, task_logits = self.m(*xs)\n",
    "        return task_logits, self.crit(lm_logits, task_logits, *xs, y)\n",
    "\n",
    "def MCLoss(lm_logits, clf_logits, X, Y, lm_coef=0.5):\n",
    "    x_shifted = X[:, :, 1:, 0].contiguous().view(-1)  # Shape: 252\n",
    "    \"\"\"M = torch.ne(X[:, :, :, 0] , T(torch.zeros(X.size()[:-1], dtype=torch.long))).to(torch.float)\n",
    "    M = M.view(-1, M.size(2))\n",
    "    lm_losses = F.cross_entropy(lm_logits, x_shifted, reduction='none')\n",
    "    lm_losses = lm_losses.view(X.size(0) * X.size(1), X.size(2) - 1)\n",
    "    lm_losses = lm_losses * M[:, 1:]\n",
    "    lm_losses = lm_losses.sum(1) / torch.sum(M[:, 1:], 1)\n",
    "    \n",
    "    clf_losses = F.cross_entropy(clf_logits, Y, reduction='none')\"\"\"\n",
    "    \n",
    "    lm_losses = F.cross_entropy(lm_logits, x_shifted, ignore_index=0, reduction='elementwise_mean')\n",
    "    clf_losses = F.cross_entropy(clf_logits, Y, reduction='sum')\n",
    "    \n",
    "    train_loss = clf_losses + lm_coef * lm_losses\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.crit = MCLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Since lr scheduling/updating is taken care of within the OpenAI optimizer, I simply tried calling learn.fit with a constant learning rate (equal to their reported learning rate), and it works as it stands here. Obviously this part will need to change if fully integrated with fast.ai's AdamW and training phase API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fde380f01b54d8bb67a3ac6a0064233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      4.834886   4.28366    0.879679  \n",
      "    1      3.453226   4.04135    0.885027                   \n",
      "    2      2.306055   4.2995     0.890374                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.299500465393066, 0.890374332188285]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(6.25e-5, 3, cycle_len=1, stepper=RocStepper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.metrics import accuracy\n",
    "def my_test(model, dl):\n",
    "    model.eval()\n",
    "    batch_cnts,loss,res = [], [], []\n",
    "    t = tqdm(iter(dl), leave=False, total=len(dl), miniters=0)\n",
    "    for inputs, targets in t:\n",
    "        lm_logits, clf_logits = model(inputs)\n",
    "        l = MCLoss(lm_logits, clf_logits, inputs, targets)\n",
    "        batch_cnts.append(inputs.size(0))\n",
    "        loss.append(to_np(l))\n",
    "        res.append([accuracy(datafy(clf_logits), datafy(targets))])\n",
    "    return [np.average(loss, 0, weights=batch_cnts)] + list(np.average(np.stack(res), 0, weights=batch_cnts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9807060644118177, 0.860502405130946]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test(learn.model, tst_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
