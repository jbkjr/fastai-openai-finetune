{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ftfy\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"\n",
    "    Return set of symbol pairs in a word.\n",
    "    word is represented as tuple of symbols (symbols being variable-length strings)\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "def text_standardize(text):\n",
    "    \"\"\"\n",
    "    fixes some issues the spacy tokenizer had on books corpus\n",
    "    also does some whitespace standardization\n",
    "    \"\"\"\n",
    "    text = text.replace('—', '-')\n",
    "    text = text.replace('–', '-')\n",
    "    text = text.replace('―', '-')\n",
    "    text = text.replace('…', '...')\n",
    "    text = text.replace('´', \"'\")\n",
    "    text = re.sub(r'''(-+|~+|!+|\"+|;+|\\?+|\\++|,+|\\)+|\\(+|\\\\+|\\/+|\\*+|\\[+|\\]+|}+|{+|\\|+|_+)''', r' \\1 ', text)\n",
    "    text = re.sub(r'\\s*\\n\\s*', ' \\n ', text)\n",
    "    text = re.sub(r'[^\\S\\n]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "class TextEncoder(object):\n",
    "    \"\"\"\n",
    "    mostly a wrapper for a public python bpe tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_path, bpe_path):\n",
    "        self.nlp = spacy.load('en', disable=['parser', 'tagger', 'ner', 'textcat'])\n",
    "        self.encoder = json.load(open(encoder_path))\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        merges = open(bpe_path, encoding='utf-8').read().split('\\n')[1:-1]\n",
    "        merges = [tuple(merge.split()) for merge in merges]\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "    def bpe(self, token):\n",
    "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token+'</w>'\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        if word == '\\n  </w>':\n",
    "            word = '\\n</w>'\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, texts, verbose=True):\n",
    "        texts_tokens = []\n",
    "        if verbose:\n",
    "            for text in tqdm(texts, ncols=80, leave=False):\n",
    "                text = self.nlp(text_standardize(ftfy.fix_text(text)))\n",
    "                text_tokens = []\n",
    "                for token in text:\n",
    "                    text_tokens.extend([self.encoder.get(t, 0) for t in self.bpe(token.text.lower()).split(' ')])\n",
    "                texts_tokens.append(text_tokens)\n",
    "        else:\n",
    "            for text in texts:\n",
    "                text = self.nlp(text_standardize(ftfy.fix_text(text)))\n",
    "                text_tokens = []\n",
    "                for token in text:\n",
    "                    text_tokens.extend([self.encoder.get(t, 0) for t in self.bpe(token.text.lower()).split(' ')])\n",
    "                texts_tokens.append(text_tokens)\n",
    "        return texts_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder('model/encoder_bpe_40000.json', 'model/vocab_40000.bpe')\n",
    "encoder = text_encoder.encoder\n",
    "n_vocab = len(text_encoder.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(*splits, encoder):\n",
    "    encoded_splits = []\n",
    "    for split in splits:\n",
    "        fields = []\n",
    "        for field in split:\n",
    "            if isinstance(field[0], str):\n",
    "                field = encoder.encode(field)\n",
    "            fields.append(field)\n",
    "        encoded_splits.append(fields)\n",
    "    return encoded_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 3535999445\n",
    "\n",
    "def _rocstories(path):\n",
    "    with open(path, encoding='utf_8') as f:\n",
    "        f = csv.reader(f)\n",
    "        st = []\n",
    "        ct1 = []\n",
    "        ct2 = []\n",
    "        y = []\n",
    "        for i, line in enumerate(tqdm(list(f), ncols=80, leave=False)):\n",
    "            if i > 0:\n",
    "                s = ' '.join(line[1:5])\n",
    "                c1 = line[5]\n",
    "                c2 = line[6]\n",
    "                st.append(s)\n",
    "                ct1.append(c1)\n",
    "                ct2.append(c2)\n",
    "                y.append(int(line[-1])-1)\n",
    "        return st, ct1, ct2, y\n",
    "\n",
    "def rocstories(data_dir, n_train=1497, n_valid=374):\n",
    "    storys, comps1, comps2, ys = _rocstories(os.path.join(data_dir, 'cloze_test_val__spring2016 - cloze_test_ALL_val.csv'))\n",
    "    teX1, teX2, teX3, _ = _rocstories(os.path.join(data_dir, 'cloze_test_test__spring2016 - cloze_test_ALL_test.csv'))\n",
    "    tr_storys, va_storys, tr_comps1, va_comps1, tr_comps2, va_comps2, tr_ys, va_ys = train_test_split(storys, comps1, comps2, ys, test_size=n_valid, random_state=seed)\n",
    "    trX1, trX2, trX3 = [], [], []\n",
    "    trY = []\n",
    "    for s, c1, c2, y in zip(tr_storys, tr_comps1, tr_comps2, tr_ys):\n",
    "        trX1.append(s)\n",
    "        trX2.append(c1)\n",
    "        trX3.append(c2)\n",
    "        trY.append(y)\n",
    "\n",
    "    vaX1, vaX2, vaX3 = [], [], []\n",
    "    vaY = []\n",
    "    for s, c1, c2, y in zip(va_storys, va_comps1, va_comps2, va_ys):\n",
    "        vaX1.append(s)\n",
    "        vaX2.append(c1)\n",
    "        vaX3.append(c2)\n",
    "        vaY.append(y)\n",
    "    trY = np.asarray(trY, dtype=np.int32)\n",
    "    vaY = np.asarray(vaY, dtype=np.int32)\n",
    "    return (trX1, trX2, trX3, trY), (vaX1, vaX2, vaX3, vaY), (teX1, teX2, teX3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ec8140cbb324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m ((trX1, trX2, trX3, trY),\n\u001b[1;32m      2\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mvaX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaX3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m  \u001b[0;34m(\u001b[0m\u001b[0mteX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteX3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrocstories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m374\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                                       encoder=text_encoder)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "((trX1, trX2, trX3, trY),\n",
    " (vaX1, vaX2, vaX3, vaY),\n",
    " (teX1, teX2, teX3)) = encode_dataset(*rocstories('data/', n_valid=374),\n",
    "                                      encoder=text_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
